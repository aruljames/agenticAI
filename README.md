ðŸš€ FastAPI + LangGraph + OpenRouter Agent

This project provides an LLM-powered API using FastAPI, LangChain / LangGraph, and OpenRouter.
It includes:

âœ… Multi-step agent workflow
âœ… Prompt transformation (â€œtoon formatâ€)
âœ… Streaming support
âœ… Docker + Docker Compose setup
âœ… Clean modular Python structure

ðŸ“ Project Structure
project/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ agent.py
â”‚   â”œâ”€â”€ graph.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ __init__.py
â”‚â”€â”€ main.py
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ docker-compose.yml
â”‚â”€â”€ Dockerfile
â”‚â”€â”€ README.md

âš™ï¸ Prerequisites

Make sure you have:

Docker installed

Docker Compose installed

An OpenRouter API Key â†’ https://openrouter.ai/settings/keys

ðŸ”§ 1. Configure Environment Variables

Edit docker-compose.yml and set:

environment:
  - OPENROUTER_API_KEY=your_key_here
  - MODEL_NAME=openrouter/model-name


Example:

environment:
  - OPENROUTER_API_KEY=sk-or-v1-xxxxx
  - MODEL_NAME=openai/gpt-4.1

ðŸ³ 2. Build & Run the Application

From the project root, run:

docker-compose up --build


The API will start at:

ðŸ‘‰ http://localhost:8000

Check docs:

ðŸ‘‰ http://localhost:8000/docs

ðŸ§ª 3. How to Test the API
â–¶ï¸ Using cURL
curl -X POST "http://localhost:8000/generate" \
    -H "Content-Type: application/json" \
    -d '{"prompt": "Tell me about space"}'

â–¶ï¸ Using Python
import requests

res = requests.post(
    "http://localhost:8000/generate",
    json={"prompt": "convert me to cartoon format"}
)
print(res.json())

â–¶ï¸ Using Browser / Swagger UI

Visit:

ðŸ‘‰ http://localhost:8000/docs

Use the POST /generate endpoint.

ðŸ§  4. How the Agent Works
When you call the API:

The input prompt is converted to toon-format

The agent processes it using LangGraph

The graph performs multi-step reasoning

The agent sends the request to OpenRouter

You receive LLM-generated output

ðŸ” 5. Streaming Support (If Enabled)

Call:

curl -N -X POST "http://localhost:8000/stream" \
    -H "Content-Type: application/json" \
    -d '{"prompt": "hello"}'


You'll receive live streaming chunks.

ðŸž 6. Debugging Tips

Check logs:

docker-compose logs -f


Restart API:

docker-compose restart


Rebuild everything:

docker-compose down
docker-compose up --build

ðŸ“ 7. Requirements File

Install locally (optional):

pip install -r requirements.txt



ðŸ§  Agent Memory (agent_memory.json)

This project uses persistent agent memory to help the LLM remember previous interactions across requests.
The memory file is stored locally as:

/data/agent_memory.json

âœ… What is stored in agent_memory.json?

This JSON file stores:

Recent conversation history

Agent internal state across graph steps

Summaries used for long-term memory

Key/value memory entries generated by the agent

Example:

{
  "history": [
    {
      "user": "Convert this text to cartoon format",
      "agent": "Sure! Here's the toon versionâ€¦"
    }
  ],
  "summary": "The user frequently asks for toon-format transformations."
}

ðŸ” Why Do We Need This File?

LLMs by default do not remember anything between requests.

Using persistent memory allows the agent to:

Maintain conversation continuity

Personalize responses

Improve long-running multi-step workflows

Allow LangGraph nodes to store internal state

This makes the agent behave more like a real assistant, not a stateless API.

ðŸ“ Where Is It Located?

Inside the Docker container:

/app/data/agent_memory.json


On your machine (mapped by docker-compose):

./data/agent_memory.json


The file is automatically created if missing.

ðŸ”„ How to Reset Agent Memory

If something feels â€œstuckâ€ or you want a fresh agent:

Method 1: Delete the file

Just remove:

rm -f data/agent_memory.json


It will be recreated automatically.

Method 2: Clear memory via endpoint

(If implemented â€” optional)

POST /reset-memory

ðŸŽ› Memory Configuration

Memory logic is located inside:

src/agent.py
src/utils.py


You can customize:

How many messages to keep

Whether to store summaries or all messages

What structure is persisted

Example (configurable):

MEMORY_FILE = "data/agent_memory.json"
MAX_HISTORY = 20

ðŸ§ª Testing Memory Behavior

Call /generate twice with related prompts

Observe that the second response references the first input

Delete agent_memory.json â†’ Test again

Now agent resets and behaves stateless

ðŸ›  Troubleshooting
Memory not updating?

Check write permissions to data/

Ensure Docker volume is mounted correctly

File missing?

The application auto-creates it on first run.

Memory growing too large?

Set limits in memory handler:

if len(memory["history"]) > MAX_HISTORY:
    memory["history"].pop(0)